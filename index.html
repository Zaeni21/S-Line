<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>S-Line Minimal</title>
  <style>
    body, html {
      margin: 0;
      overflow: hidden;
      background: #000;
    }
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
    }
    #switchBtn {
      position: absolute;
      top: 10px;
      left: 10px;
      z-index: 10;
      background: rgba(0,0,0,0.5);
      color: #fff;
      border: 1px solid #fff;
      padding: 6px 12px;
      border-radius: 4px;
      font-size: 14px;
    }
  </style>
</head>
<body>
  <button id="switchBtn">Switch Camera</button>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.18.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.3"></script>
  <script>
    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const switchBtn = document.getElementById("switchBtn");

    let usingFront = true;
    let model, head = null;
    const numLines = Math.floor(Math.random() * 5) + 3;

    async function setupCamera(facingMode = "user") {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode },
        audio: false
      });
      video.srcObject = stream;
      await video.play();
    }

    async function loadModel() {
      await tf.setBackend("webgl");
      model = await faceLandmarksDetection.load(
        faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
      );
    }

    async function detectFace() {
      if (!model || video.readyState !== 4) return;
      const predictions = await model.estimateFaces({ input: video });
      if (predictions.length > 0) {
        const nose = predictions[0].scaledMesh[10]; // dahi
        const w = canvas.width;
        const h = canvas.height;
        head = {
          x: (nose[0] / 640) * w,
          y: (nose[1] / 480) * h
        };
      } else {
        head = null;
      }
    }

    function draw() {
      canvas.width = window.innerWidth;
      canvas.height = window.innerHeight;
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      if (!head) return;

      const spread = Math.PI / 1.5;
      for (let i = 0; i < numLines; i++) {
        const angle = -spread / 2 + (i / (numLines - 1)) * spread;
        const length = canvas.height * 0.8;
        const cx = head.x + Math.sin(angle + i) * 20;
        const cy = head.y - length;

        ctx.beginPath();
        ctx.moveTo(head.x, head.y);
        ctx.bezierCurveTo(head.x, head.y - 40, cx, cy + 40, cx, cy);
        ctx.strokeStyle = "rgba(255, 0, 0, 0.8)";
        ctx.lineWidth = 2;
        ctx.stroke();
      }

      ctx.fillStyle = "red";
      ctx.beginPath();
      ctx.arc(head.x, head.y, 6, 0, Math.PI * 2);
      ctx.fill();
    }

    async function loop() {
      await detectFace();
      draw();
      requestAnimationFrame(loop);
    }

    switchBtn.onclick = async () => {
      usingFront = !usingFront;
      const mode = usingFront ? "user" : "environment";
      await setupCamera(mode);
    };

    (async () => {
      await setupCamera("user");
      await loadModel();
      loop();
    })();
  </script>
</body>
</html>
